apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app.kubernetes.io/name: wiztelemetry
    app.kubernetes.io/part-of: wiztelemetry
    prometheus: k8s
    role: alert-rules
  name: wiztelemetry-rules
  namespace: kubesphere-monitoring-system
spec:
  groups:
  - name: wiztelemetry-cluster.rules
    rules:
    - expr: |
        max by (cluster, node, ip, workspace, namespace, pod, qos_class, phase, workload, workload_type) (
                    kube_pod_info{job="kube-state-metrics"}
                  * on (cluster, namespace, pod) group_left (qos_class)
                    max by (cluster, namespace, pod, qos_class) (
                      kube_pod_status_qos_class{job="kube-state-metrics"} > 0
                    )
                * on (cluster, namespace, pod) group_left (ip)
                  max by (cluster, namespace, pod, ip) (
                      kube_pod_ips{job="kube-state-metrics"}
                    or
                        sum by (pod, namespace, cluster) (kube_pod_info{job="kube-state-metrics"})
                      unless
                        sum by (pod, namespace, cluster) (kube_pod_ips{job="kube-state-metrics"})
                  )
              * on (cluster, namespace, pod) group_left (phase)
                max by (cluster, namespace, pod, phase) (kube_pod_status_phase{job="kube-state-metrics"} > 0)
            * on (cluster, namespace, pod) group_left (workload, workload_type)
              max by (cluster, namespace, pod, workload, workload_type) (
                  label_join(
                    label_join(
                      kube_pod_owner{job="kube-state-metrics",owner_kind!~"ReplicaSet|DaemonSet|StatefulSet|Job"},
                      "workload",
                      "$1",
                      "owner_name"
                    ),
                    "workload_type",
                    "$1",
                    "owner_kind"
                  )
                or
                    kube_pod_owner{job="kube-state-metrics",owner_kind=~"ReplicaSet|DaemonSet|StatefulSet|Job"}
                  * on (cluster, namespace, pod) group_left (workload_type, workload)
                    namespace_workload_pod:kube_pod_owner:relabel
              )
          * on (cluster, namespace) group_left (workspace)
            max by (cluster, namespace, workspace) (kube_namespace_labels{job="kube-state-metrics"})
        )
      record: 'workspace_workload_node:kube_pod_info:'
    - expr: |
        label_replace(
          max by (cluster, node, role, internal_ip) (
                kube_node_info{job="kube-state-metrics"}
              * on (cluster, node) group_left (role)
                max by (cluster, node, role) (
                        (
                            kube_node_role{job="kube-state-metrics",role="worker"}
                          unless ignoring (role)
                            kube_node_role{job="kube-state-metrics",role=~"control-plane|edge"}
                        )
                      or
                        (
                            kube_node_role{job="kube-state-metrics",role="control-plane"}
                          unless ignoring (role)
                            kube_node_role{job="kube-state-metrics",role="edge"}
                        )
                    or
                      kube_node_role{job="kube-state-metrics",role="edge"}
                  or
                    topk by (cluster, node) (
                      1,
                        kube_node_role{job="kube-state-metrics"}
                      unless ignoring (role)
                        (kube_node_role{job="kube-state-metrics",role=~"edge|control-plane|worker|fakenode"})
                    )
                )
            or
                kube_node_info{job="kube-state-metrics"}
              unless on (cluster, node)
                kube_node_role{job="kube-state-metrics"}
          ),
          "host_ip",
          "$1",
          "internal_ip",
          "(.*)"
        )
      record: 'node_role_ip:kube_node_info:'
    - expr: |
        max by (cluster, namespace, pod, pod_ip, host_ip, node) (
              kube_pod_info{job="kube-state-metrics"}
            * on (cluster, namespace, pod, uid) group_left ()
              kube_pod_start_time{job="kube-state-metrics"}
          )
        * on (cluster, namespace, pod) group_left (workspace, qos_class, phase)
          workspace_workload_node:kube_pod_info:
      record: 'pod_start_time:kube_pod_info:'
  - name: wiztelemetry-node.rules
    rules:
    - expr: |
        sum by (cluster, node) (
            avg by (cluster, instance, namespace, pod) (
              sum without (mode) (
                rate(node_cpu_seconds_total{job="node-exporter",mode!="idle",mode!="iowait",mode!="steal"}[5m])
              )
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_cpu_utilization:ratio
    - expr: |
        node:node_cpu_utilization:ratio * node:node_num_cpu:sum
      record: node:node_cpu_utilization:sum
    - expr: |
        node:node_memory_used_bytes:sum / node:node_memory_bytes_total:sum
      record: node:node_memory_utilisation:ratio
    - expr: |
        node:node_memory_bytes_total:sum - node:node_memory_used_bytes:sum
      record: node:node_memory_available_bytes:sum
    - expr: |
        sum by (cluster, node) (
            (
                node_memory_MemTotal_bytes{job="node-exporter"}
              -
                (
                    node_memory_MemAvailable_bytes{job="node-exporter"}
                  or
                    (
                          node_memory_Buffers_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"}
                        +
                          node_memory_MemFree_bytes{job="node-exporter"}
                      +
                        node_memory_Slab_bytes{job="node-exporter"}
                    )
                )
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_memory_used_bytes:sum
    - expr: |
        sum by (cluster, node) (
            node_memory_MemTotal_bytes{job="node-exporter"}
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_memory_bytes_total:sum
    - expr: |
        node:node_device_filesystem_used_bytes:sum / node:node_device_filesystem_bytes_total:sum
      record: node:node_device_filesystem_utilisation:ratio
    - expr: |
        sum by (cluster, node, device) (
              max by (cluster, namespace, pod, instance, device) (
                  node_filesystem_avail_bytes{device=~"/dev/.*",device!~"/dev/loop\\d+", job="node-exporter"}
              )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_device_filesystem_available_bytes:sum
    - expr: |
        sum by (cluster, node, device) (
              max by (cluster, namespace, pod, instance, device) (
                  node_filesystem_size_bytes{device=~"/dev/.*",device!~"/dev/loop\\d+", job="node-exporter"}
                -
                  node_filesystem_avail_bytes{device=~"/dev/.*",device!~"/dev/loop\\d+", job="node-exporter"}
              )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_device_filesystem_used_bytes:sum
    - expr: |
        sum by (cluster, node, device) (
              max by (cluster, namespace, pod, instance, device) (
                  node_filesystem_size_bytes{device=~"/dev/.*",device!~"/dev/loop\\d+", job="node-exporter"}
              )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_device_filesystem_bytes_total:sum
    - expr: |
        node:node_filesystem_used_bytes:sum / node:node_filesystem_bytes_total:sum
      record: node:node_filesystem_utilisation:ratio
    - expr: |
        sum by (cluster, node)(node:node_device_filesystem_available_bytes:sum)
      record: node:node_filesystem_available_bytes:sum
    - expr: |
        sum by (cluster, node)(node:node_device_filesystem_used_bytes:sum)
      record: node:node_filesystem_used_bytes:sum
    - expr: |
        sum by (cluster, node)(node:node_device_filesystem_bytes_total:sum)
      record: node:node_filesystem_bytes_total:sum
    - expr: |
        node:node_pod_running_total:sum / node:node_pod_quota:sum
      record: node:node_pod_utilisation:ratio
    - expr: |
        count by(cluster, node) (
            node_namespace_pod:kube_pod_info:
            unless on (cluster, namespace, pod)
            (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed|Pending|Unknown|Succeeded"} > 0)
        )
      record: node:node_pod_running_total:sum
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, namespace, pod) (kube_pod_status_scheduled{job="kube-state-metrics"} > 0)
          * on (cluster, namespace, pod) group_left (node)
            node_namespace_pod:kube_pod_info:
        )
      record: node:node_pod_total:sum
    - expr: |
        sum by (cluster, node) (kube_node_status_allocatable{job="kube-state-metrics",resource="pods"})
      record: node:node_pod_quota:sum
    - expr: |
        count by (cluster, node) (
                  node_namespace_pod:kube_pod_info:{node!=""}
                unless on (pod, namespace, cluster)
                  (kube_pod_status_phase{job="kube-state-metrics",phase="Succeeded"} > 0)
              unless on (pod, namespace, cluster)
                (
                    (kube_pod_status_ready{condition="true",job="kube-state-metrics"} > 0)
                  and on (pod, namespace, cluster)
                    (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} > 0)
                )
            unless on (cluster, pod, namespace)
              kube_pod_container_status_waiting_reason{job="kube-state-metrics",reason="ContainerCreating"} > 0
        )
        /
        count by (cluster, node) (
              node_namespace_pod:kube_pod_info:{node!=""}
            unless on (pod, namespace, cluster)
              kube_pod_status_phase{job="kube-state-metrics",phase="Succeeded"} > 0
        )
      record: node:pod_abnormal_utilisation:ratio
    - expr: |
        sum by (cluster, node)(node_load1{job="node-exporter"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)) / node:node_num_cpu:sum
      record: node:node_load1_per_cpu:ratio
    - expr: |
        sum by (cluster, node)(node_load5{job="node-exporter"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)) / node:node_num_cpu:sum
      record: node:node_load5_per_cpu:ratio
    - expr: |
        sum by (cluster, node)(node_load15{job="node-exporter"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)) / node:node_num_cpu:sum
      record: node:node_load15_per_cpu:ratio
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_disk_read_bytes_total{job="node-exporter"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:data_volume_iops_reads:sum
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_disk_writes_completed_total{job="node-exporter"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:data_volume_iops_writes:sum
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_disk_read_bytes_total{job="node-exporter"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:data_volume_throughput_read_bytes:sum_irate
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_disk_written_bytes_total{job="node-exporter"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:data_volume_throughput_written_bytes:sum_irate
    - expr: |
        node:node_inodes_used_total:sum / node:node_inodes_total:sum
      record: node:node_inodes_utilisation:ratio
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              node_filesystem_files{job="node-exporter", device=~"/dev/.*",device!~"/dev/loop\\d+"}
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_inodes_total:sum
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              node_filesystem_files{job="node-exporter", device=~"/dev/.*",device!~"/dev/loop\\d+"} - node_filesystem_files_free{job="node-exporter", device=~"/dev/.*",device!~"/dev/loop\\d+"}
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_inodes_used_total:sum
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_network_transmit_bytes_total{job="node-exporter", device!~"veth.+"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_net_transmit_bytes:sum_irate
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_network_receive_bytes_total{job="node-exporter", device!~"veth.+"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_net_receive_bytes:sum_irate
  - name: wiztelemetry-workload.rules
    rules:
    - expr: |
        sum by (cluster, namespace, workload, workload_type) (
            sum by (cluster, namespace, pod) (
              node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
            )
          * on (cluster, namespace, pod) group_left (workload, workload_type)
            workspace_workload_node:kube_pod_info:
        )
      record: namespace:workload_cpu_usage:sum
    - expr: |
        sum by (cluster, namespace, workload, workload_type) (
            sum by (cluster, namespace, pod) (
              container_memory_usage_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
            )
          * on (cluster, namespace, pod) group_left (workload, workload_type)
            workspace_workload_node:kube_pod_info:
        )
      record: namespace:workload_memory_usage:sum
    - expr: |
        sum by (cluster, namespace, workload, workload_type) (
            sum by (cluster, namespace, pod) (
              node_namespace_pod_container:container_memory_working_set_bytes
            )
          * on (cluster, namespace, pod) group_left (workload, workload_type)
            workspace_workload_node:kube_pod_info:
        )
      record: namespace:workload_memory_wo_cache_usage:sum
    - expr: |
        sum by (cluster, namespace, workload, workload_type) (
            sum by (cluster, namespace, pod) (
              irate(container_network_receive_bytes_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
            )
          * on (cluster, namespace, pod) group_left (workload, workload_type)
            workspace_workload_node:kube_pod_info:
        )
      record: namespace:workload_net_receive_bytes:sum_irate
    - expr: |
        sum by (cluster, namespace, workload, workload_type) (
            sum by (cluster, namespace, pod) (
              irate(container_network_transmit_bytes_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
            )
          * on (cluster, namespace, pod) group_left (workload, workload_type)
            workspace_workload_node:kube_pod_info:
        )
      record: namespace:workload_net_transmit_bytes:sum_irate
    - expr: |
        label_replace(sum(kube_daemonset_status_number_unavailable{job="kube-state-metrics"}) by (daemonset, namespace, cluster) / sum(kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}) by (daemonset, namespace,cluster), "workload", "$1", "daemonset", "(.*)")
      labels:
        workload_type: daemonset
      record: namespace:workload_unavailable_replicas:ratio
    - expr: |
        label_replace(sum(kube_deployment_status_replicas_unavailable{job="kube-state-metrics"}) by (deployment, namespace, cluster) / sum(kube_deployment_spec_replicas{job="kube-state-metrics"}) by (deployment, namespace, cluster), "workload", "$1", "deployment", "(.*)")
      labels:
        workload_type: deployment
      record: namespace:workload_unavailable_replicas:ratio
    - expr: |
        label_replace(1 - sum(kube_statefulset_status_replicas_ready{job="kube-state-metrics"}) by (statefulset, namespace, cluster) / sum(kube_statefulset_status_replicas{job="kube-state-metrics"}) by (statefulset, namespace, cluster), "workload", "$1", "statefulset", "(.*)")
      labels:
        workload_type: statefulset
      record: namespace:workload_unavailable_replicas:ratio
    - expr: |
        label_replace(kube_deployment_created{job="kube-state-metrics"},"workload", "$1", "deployment", "(.*)")
      labels:
        workload_type: deployment
      record: namespace_workload:workload_created:relabel
    - expr: |
        label_replace(kube_deployment_spec_replicas{job="kube-state-metrics"},"workload", "$1", "deployment", "(.*)")
      labels:
        workload_type: deployment
      record: namespace_workload:workload_replicas:relabel
    - expr: |
        label_replace(kube_deployment_status_replicas_ready{job="kube-state-metrics"},"workload", "$1", "deployment", "(.*)")
      labels:
        workload_type: deployment
      record: namespace_workload:workload_replicas_ready:relabel
    - expr: |
        label_replace(kube_statefulset_created{job="kube-state-metrics"},"workload", "$1", "statefulset", "(.*)")
      labels:
        workload_type: statefulset
      record: namespace_workload:workload_created:relabel
    - expr: |
        label_replace(kube_statefulset_replicas{job="kube-state-metrics"},"workload", "$1", "statefulset", "(.*)")
      labels:
        workload_type: statefulset
      record: namespace_workload:workload_replicas:relabel
    - expr: |
        label_replace(kube_statefulset_status_replicas_ready{job="kube-state-metrics"},"workload", "$1", "statefulset", "(.*)")
      labels:
        workload_type: statefulset
      record: namespace_workload:workload_replicas_ready:relabel
    - expr: |
        label_replace(kube_daemonset_created{job="kube-state-metrics"},"workload", "$1", "daemonset", "(.*)")
      labels:
        workload_type: daemonset
      record: namespace_workload:workload_created:relabel
    - expr: |
        label_replace(kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"},"workload", "$1", "daemonset", "(.*)")
      labels:
        workload_type: daemonset
      record: namespace_workload:workload_replicas:relabel
    - expr: |
        label_replace(kube_daemonset_status_number_ready{job="kube-state-metrics"},"workload", "$1", "daemonset", "(.*)")
      labels:
        workload_type: daemonset
      record: namespace_workload:workload_replicas_ready:relabel
  - name: wiztelemetry-pod.rules
    rules:
    - expr: |
        sum by (cluster, namespace, pod) (
            irate(
              container_network_receive_bytes_total{image!="",job="kubelet",metrics_path="/metrics/cadvisor"}[5m]
            )
          )
        * on (cluster, namespace, pod) group_left (node)
          topk by (cluster, namespace, pod) (
            1,
            max by (cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
      record: node_namespace_pod:container_network_receive_bytes_total:sum_irate
    - expr: |
        sum by (cluster, namespace, pod) (
            irate(
              container_network_transmit_bytes_total{image!="",job="kubelet",metrics_path="/metrics/cadvisor"}[5m]
            )
          )
        * on (cluster, namespace, pod) group_left (node)
          topk by (cluster, namespace, pod) (
            1,
            max by (cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
      record: node_namespace_pod:container_network_transmit_bytes_total:sum_irate
  - name: wiztelemetry-apiserver.rules
    rules:
    - expr: |
        sum by(cluster) (irate(apiserver_request_total{job="apiserver"}[5m]))
      record: apiserver:apiserver_request_total:sum_irate
    - expr: |
        sum by (cluster, verb)(irate(apiserver_request_total{job="apiserver"}[5m]))
      record: apiserver:apiserver_request_total:sum_verb_irate
    - expr: |
        sum by(cluster) (irate(apiserver_request_duration_seconds_sum{job="apiserver",subresource!="log", verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) / sum by(cluster) (irate(apiserver_request_duration_seconds_count{job="apiserver", subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m]))
      record: apiserver:apiserver_request_duration:avg
    - expr: "sum by (cluster, verb)(irate(apiserver_request_duration_seconds_sum{job=\"apiserver\",subresource!=\"log\", verb!~\"LIST|WATCH|WATCHLIST|PROXY|CONNECT\"}[5m]))  / sum by (cluster, verb)(irate(apiserver_request_duration_seconds_count{job=\"apiserver\", subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|PROXY|CONNECT\"}[5m])) \n"
      record: apiserver:apiserver_request_duration:avg_by_verb
  - name: wiztelemetry-etcd.rules
    rules:
    - expr: |
        sum by(cluster) (up{job=~".*etcd.*"} == 1)
      record: etcd:up:sum
    - expr: |
        sum(label_replace(sum(changes(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}[1h])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_server_leader_changes_seen:sum_changes
    - expr: |
        sum(label_replace(sum(irate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_server_proposals_failed:sum_irate
    - expr: |
        sum(label_replace(sum(irate(etcd_server_proposals_applied_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_server_proposals_applied:sum_irate
    - expr: |
        sum(label_replace(sum(irate(etcd_server_proposals_committed_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_server_proposals_committed:sum_irate
    - expr: |
        sum(label_replace(sum(etcd_server_proposals_pending{job=~".*etcd.*"}) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_server_proposals_pending:sum
    - expr: |
        sum(label_replace(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"},"node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_mvcc_db_total_size:sum
    - expr: |
        sum(label_replace(sum(irate(etcd_network_client_grpc_received_bytes_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_network_client_grpc_received_bytes:sum_irate
    - expr: |
        sum(label_replace(sum(irate(etcd_network_client_grpc_sent_bytes_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_network_client_grpc_sent_bytes:sum_irate
    - expr: |
        sum(label_replace(sum(irate(grpc_server_started_total{job=~".*etcd.*",grpc_type="unary"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:grpc_server_started:sum_irate
    - expr: |
        sum(label_replace(sum(irate(grpc_server_handled_total{job=~".*etcd.*",grpc_type="unary",grpc_code!="OK"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:grpc_server_handled:sum_irate
    - expr: |
        sum(label_replace(sum(irate(grpc_server_msg_received_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:grpc_server_msg_received:sum_irate
    - expr: |
        sum(label_replace(sum(irate(grpc_server_msg_sent_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:grpc_server_msg_sent:sum_irate
    - expr: |
        sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_sum{job=~".*etcd.*"}[5m])) by (instance, cluster) / sum(irate(etcd_disk_wal_fsync_duration_seconds_count{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_disk_wal_fsync_duration:avg
    - expr: |
        sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_sum{job=~".*etcd.*"}[5m])) by (instance, cluster) / sum(irate(etcd_disk_backend_commit_duration_seconds_count{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_disk_backend_commit_duration:avg
    - expr: |
        histogram_quantile(0.99, sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.99"
      record: etcd:etcd_disk_wal_fsync_duration:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.9"
      record: etcd:etcd_disk_wal_fsync_duration:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.5"
      record: etcd:etcd_disk_wal_fsync_duration:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.99"
      record: etcd:etcd_disk_backend_commit_duration:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.9"
      record: etcd:etcd_disk_backend_commit_duration:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.5"
      record: etcd:etcd_disk_backend_commit_duration:histogram_quantile
  - name: wiztelemetry-kube-scheduler.rules
    rules:
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_scheduling_attempt_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_scheduling_attempt_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_scheduling_attempt_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_scheduling_attempt_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_scheduling_attempt_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_scheduling_attempt_duration_seconds:histogram_quantile
    - expr: |
        sum by(cluster) (rate(scheduler_e2e_scheduling_duration_seconds_sum{job="kube-scheduler"}[5m])) / sum by(cluster) (rate(scheduler_e2e_scheduling_duration_seconds_count{job="kube-scheduler"}[5m]))
      record: cluster:scheduler_e2e_scheduling_duration_seconds:avg
    - expr: |
        sum by(cluster) (rate(scheduler_scheduling_attempt_duration_seconds_sum{job="kube-scheduler"}[5m])) / sum by(cluster) (rate(scheduler_scheduling_attempt_duration_seconds_count{job="kube-scheduler"}[5m]))
      record: cluster:scheduler_scheduling_attempt_duration_seconds:avg
  - name: wiztelemetry-cambricon-mlu.rules
    rules:
    - expr: |
        sum by (cluster, node, namespace, pod, container) (
            mlu_utilization{job="mlu-monitoring"} / 100
          * on (cluster, uuid) group_left (namespace, pod, container, node)
            mlu_container{job="mlu-monitoring"}
        )
      record: node_namespace_pod_container:container_gpu_utilization
    - expr: |
        sum by (cluster, node, namespace, pod, container) (
            mlu_memory_used{job="mlu-monitoring"}
          * on (cluster, uuid) group_left (namespace, pod, container, node)
            mlu_container{job="mlu-monitoring"}
        )
      record: node_namespace_pod_container:container_gpu_memory_usage
    - expr: |
        label_replace(
          label_replace(mlu_temperature{job="mlu-monitoring"}, "device_num", "mlu${1}", "mlu", "(.*)"),
          "device_name",
          "$1",
          "model",
          "(.*)"
        )
      record: node:gpu_device:gpu_temperature
    - expr: |
        label_replace(
          label_replace(mlu_power_usage{job="mlu-monitoring"}, "device_num", "mlu${1}", "mlu", "(.*)"),
          "device_name",
          "$1",
          "model",
          "(.*)"
        )
      record: node:gpu_device:gpu_power_usage
    - expr: |
        label_replace(
          label_replace(mlu_memory_used{job="mlu-monitoring"}, "device_num", "mlu${1}", "mlu", "(.*)"),
          "device_name",
          "$1",
          "model",
          "(.*)"
        )
      record: node:gpu_device:gpu_memory_used_bytes
    - expr: |
        label_replace(
          label_replace(mlu_memory_total{job="mlu-monitoring"}, "device_num", "mlu${1}", "mlu", "(.*)"),
          "device_name",
          "$1",
          "model",
          "(.*)"
        )
      record: node:gpu_device:gpu_memory_total_bytes
    - expr: |
        label_replace(
          label_replace(mlu_memory_utilization{job="mlu-monitoring"} / 100, "device_num", "mlu${1}", "mlu", "(.*)"),
          "device_name",
          "$1",
          "model",
          "(.*)"
        )
      record: node:gpu_device:gpu_memory_utilization
    - expr: |
        label_replace(
          label_replace(mlu_utilization{job="mlu-monitoring"} / 100, "device_num", "mlu${1}", "mlu", "(.*)"),
          "device_name",
          "$1",
          "model",
          "(.*)"
        )
      record: node:gpu_device:gpu_utilization
    - expr: |
        sum by (cluster, node) (
          kube_pod_container_resource_requests{job="kube-state-metrics",resource="cambricon_com_mlu370"}
        )
      record: node:node_gpu_allocated_num:sum
    - expr: |
        sum by(cluster, node) (
            kube_node_status_allocatable{job="kube-state-metrics",resource="cambricon_com_mlu370"}
        )
      record: node:node_gpu_num:sum
  - name: wiztelemetry-nvidia-gpu.rules
    rules:
    - expr: |
        sum by (cluster, namespace, name, pod, container, node) (
          label_replace(
              DCGM_FI_DEV_GPU_UTIL{job="nvidia-dcgm-exporter", exported_namespace="",exported_pod=""} / 100
            or
              label_replace(
                label_replace(
                  DCGM_FI_DEV_GPU_UTIL{job="nvidia-dcgm-exporter", exported_namespace!="",exported_pod!=""} / 100,
                  "namespace",
                  "$1",
                  "exported_namespace",
                  "(.*)"
                ),
                "pod",
                "$1",
                "exported_pod",
                "(.*)"
              ),
            "node",
            "$1",
            "Hostname",
            "(.*)"
          )
        )
      record: node_namespace_pod_container:container_gpu_utilization
    - expr: |
        sum by (cluster, namespace, name, pod, container, node) (
          label_replace(
              DCGM_FI_DEV_FB_USED{job="nvidia-dcgm-exporter", exported_namespace="",exported_pod=""} * 1024 * 1024
            or
              label_replace(
                label_replace(
                  DCGM_FI_DEV_FB_USED{job="nvidia-dcgm-exporter", exported_namespace!="",exported_pod!=""} * 1024 * 1024,
                  "namespace",
                  "$1",
                  "exported_namespace",
                  "(.*)"
                ),
                "pod",
                "$1",
                "exported_pod",
                "(.*)"
              ),
            "node",
            "$1",
            "Hostname",
            "(.*)"
          )
        )
      record: node_namespace_pod_container:container_gpu_memory_usage
    - expr: |
        label_replace(
          label_replace(
            label_replace(
              DCGM_FI_DEV_GPU_TEMP{job="nvidia-dcgm-exporter"},
              "device_num",
              "gpu${1}",
              "gpu",
              "(.*)"
            ),
            "device_name",
            "$1",
            "DCGM_FI_DEV_NAME",
            "(.*)"
          ),
          "node",
          "$1",
          "Hostname",
          "(.*)"
        )
      record: node:gpu_device:gpu_temperature
    - expr: |
        label_replace(
          label_replace(
            label_replace(
              DCGM_FI_DEV_POWER_USAGE{job="nvidia-dcgm-exporter"},
              "device_num",
              "gpu${1}",
              "gpu",
              "(.*)"
            ),
            "device_name",
            "$1",
            "DCGM_FI_DEV_NAME",
            "(.*)"
          ),
          "node",
          "$1",
          "Hostname",
          "(.*)"
        )
      record: node:gpu_device:gpu_power_usage
    - expr: |
        label_replace(
          label_replace(
            label_replace(
              DCGM_FI_DEV_FB_USED{job="nvidia-dcgm-exporter"} * 1024 * 1024,
              "device_num",
              "gpu${1}",
              "gpu",
              "(.*)"
            ),
            "device_name",
            "$1",
            "DCGM_FI_DEV_NAME",
            "(.*)"
          ),
          "node",
          "$1",
          "Hostname",
          "(.*)"
        )
      record: node:gpu_device:gpu_memory_used_bytes
    - expr: |
        label_replace(
          label_replace(
            label_replace(
              (DCGM_FI_DEV_FB_USED{job="nvidia-dcgm-exporter"} + DCGM_FI_DEV_FB_FREE{job="nvidia-dcgm-exporter"}) * 1024 * 1024,
              "device_num",
              "gpu${1}",
              "gpu",
              "(.*)"
            ),
            "device_name",
            "$1",
            "DCGM_FI_DEV_NAME",
            "(.*)"
          ),
          "node",
          "$1",
          "Hostname",
          "(.*)"
        )
      record: node:gpu_device:gpu_memory_total_bytes
    - expr: |
        label_replace(
          label_replace(
            label_replace(
              (DCGM_FI_DEV_FB_USED{job="nvidia-dcgm-exporter"} / (DCGM_FI_DEV_FB_USED{job="nvidia-dcgm-exporter"} + DCGM_FI_DEV_FB_FREE{job="nvidia-dcgm-exporter"})),
              "device_num",
              "gpu${1}",
              "gpu",
              "(.*)"
            ),
            "device_name",
            "$1",
            "DCGM_FI_DEV_NAME",
            "(.*)"
          ),
          "node",
          "$1",
          "Hostname",
          "(.*)"
        )
      record: node:gpu_device:gpu_memory_utilization
    - expr: |
        label_replace(
          label_replace(
            label_replace(
              DCGM_FI_DEV_GPU_UTIL{job="nvidia-dcgm-exporter"} / 100,
              "device_num",
              "gpu${1}",
              "gpu",
              "(.*)"
            ),
            "device_name",
            "$1",
            "DCGM_FI_DEV_NAME",
            "(.*)"
          ),
          "node",
          "$1",
          "Hostname",
          "(.*)"
        )
      record: node:gpu_device:gpu_utilization
    - expr: |
        sum by (cluster, node) (
          kube_pod_container_resource_requests{job="kube-state-metrics",resource="nvidia_com_gpu"}
        )
      record: node:node_gpu_allocated_num:sum
    - expr: |
        sum by(cluster, node) (
            kube_node_status_allocatable{job="kube-state-metrics",resource="nvidia_com_gpu"}
        )
      record: node:node_gpu_num:sum
  - name: wiztelemetry-ascend-npu.rules
    rules:
    - expr: |
        sum by (cluster, namespace, pod, container, node) (
          label_replace(
            label_replace(
              label_replace(
                container_npu_utilization{container_name!="",exported_namespace!="",job="npu-exporter",pod_name!=""} / 100,
                "container",
                "$1",
                "container_name",
                "(.*)"
              ),
              "pod",
              "$1",
              "pod_name",
              "(.*)"
            ),
            "namespace",
            "$1",
            "exported_namespace",
            "(.*)"
          )
        )
      record: node_namespace_pod_container:container_gpu_utilization
    - expr: |
        sum by (cluster, namespace, pod, container, node) (
          label_replace(
            label_replace(
              label_replace(
                container_npu_used_memory{container_name!="",exported_namespace!="",job="npu-exporter",pod_name!=""} * 1024 * 1024,
                "container",
                "$1",
                "container_name",
                "(.*)"
              ),
              "pod",
              "$1",
              "pod_name",
              "(.*)"
            ),
            "namespace",
            "$1",
            "exported_namespace",
            "(.*)"
          )
        )
      record: node_namespace_pod_container:container_gpu_memory_usage
    - expr: |
        label_replace(
          label_replace(npu_chip_info_temperature{job="npu-exporter"}, "device_num", "npu${1}", "id", "(.*)"),
          "device_name",
          "$1",
          "model_name",
          "(.*)"
        )
      record: node:gpu_device:gpu_temperature
    - expr: |
        label_replace(
          label_replace(npu_chip_info_power{job="npu-exporter"}, "device_num", "npu${1}", "id", "(.*)"),
          "device_name",
          "$1",
          "model_name",
          "(.*)"
        )
      record: node:gpu_device:gpu_power_usage
    - expr: |
        label_replace(
          label_replace(npu_chip_info_used_memory{job="npu-exporter"}, "device_num", "npu${1}", "id", "(.*)") * 1024 * 1024,
          "device_name",
          "$1",
          "model_name",
          "(.*)"
        )
      record: node:gpu_device:gpu_memory_used_bytes
    - expr: |
        label_replace(
          label_replace(npu_chip_info_total_memory{job="npu-exporter"}, "device_num", "npu${1}", "id", "(.*)") * 1024 * 1024,
          "device_name",
          "$1",
          "model_name",
          "(.*)"
        )
      record: node:gpu_device:gpu_memory_total_bytes
    - expr: |
        label_replace(
          label_replace(
            npu_chip_info_used_memory{job="npu-exporter"} / npu_chip_info_total_memory{job="npu-exporter"},
            "device_num",
            "npu${1}",
            "id",
            "(.*)"
          ),
          "device_name",
          "$1",
          "model_name",
          "(.*)"
        )
      record: node:gpu_device:gpu_memory_utilization
    - expr: |
        label_replace(
          label_replace(npu_chip_info_utilization{job="npu-exporter"}, "device_num", "npu${1}", "id", "(.*)") / 100,
          "device_name",
          "$1",
          "model_name",
          "(.*)"
        )
      record: node:gpu_device:gpu_utilization
    - expr: |
        sum by (cluster, node) (
          kube_pod_container_resource_requests{job="kube-state-metrics",resource=~"huawei_com_Ascend(.*)"}
        )
      record: node:node_gpu_allocated_num:sum
    - expr: |
        sum by(cluster, node) (
            kube_node_status_allocatable{job="kube-state-metrics",resource=~"huawei_com_Ascend(.*)"}
        )
      record: node:node_gpu_num:sum
  - name: wiztelemetry-kubelet.rules
    rules:
    - expr: |
        histogram_quantile(0.99, sum by (cluster, instance, le)(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet"}[5m])) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet"})
      labels:
        quantile: "0.99"
      record: node_quantile:kubelet_pod_worker_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum by (cluster, instance, operation_type, le)(rate(kubelet_runtime_operations_duration_seconds_bucket{job="kubelet"}[5m])) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet"})
      labels:
        quantile: "0.99"
      record: node_quantile:kubelet_runtime_operations_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum by (cluster, instance, operation_name, volume_plugin, le)(rate(storage_operation_duration_seconds_bucket{job="kubelet"}[5m])) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet"})
      labels:
        quantile: "0.99"
      record: node_quantile:storage_operation_duration_seconds:histogram_quantile
